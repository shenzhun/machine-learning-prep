{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day22_three_ways_of_training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvlvvLlSQ8wzQWBXDajo+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shenzhun/machine-learning-prep/blob/master/tensorflow/day22_three_ways_of_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M62ZSmi6--Y"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEL0M-wY6Blp",
        "outputId": "5d76dcc4-9ffa-457c-ffdb-eee18ec680be"
      },
      "source": [
        "MAX_LEN = 300\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.reuters.load_data()\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
        "\n",
        "MAX_WORDS = x_train.max() + 1\n",
        "CAT_NUM = y_train.max() + 1\n",
        "\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
        "          .shuffle(buffer_size=1000).batch(BATCH_SIZE) \\\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
        "\n",
        "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
        "          .shuffle(buffer_size=1000).batch(BATCH_SIZE) \\\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE).cache()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOOC0D4p8e5K"
      },
      "source": [
        "1. Pre-defined fit method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_7OFiEN8Y-I",
        "outputId": "499e8694-d063-4416-ad7d-d43486542cf7"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def create_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN))\n",
        "  model.add(layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\"))\n",
        "  model.add(layers.MaxPool1D(2))\n",
        "  model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "  model.add(layers.MaxPool1D(2))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(CAT_NUM, activation=\"softmax\"))\n",
        "  return model\n",
        "\n",
        "def compile_model(model):\n",
        "  model.compile(optimizer=optimizers.Nadam(),\n",
        "                loss=losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=[metrics.SparseCategoricalAccuracy(), metrics.SparseTopKCategoricalAccuracy(5)])\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "model = compile_model(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89GRcBUPAQTo",
        "outputId": "49645ba4-acfd-48a9-ca78-4e66b757411f"
      },
      "source": [
        "history = model.fit(ds_train, validation_data=ds_test, epochs=10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "281/281 [==============================] - 10s 31ms/step - loss: 2.3921 - sparse_categorical_accuracy: 0.3929 - sparse_top_k_categorical_accuracy: 0.7140 - val_loss: 1.6668 - val_sparse_categorical_accuracy: 0.5681 - val_sparse_top_k_categorical_accuracy: 0.7627\n",
            "Epoch 2/10\n",
            "281/281 [==============================] - 9s 31ms/step - loss: 1.5753 - sparse_categorical_accuracy: 0.5953 - sparse_top_k_categorical_accuracy: 0.7774 - val_loss: 1.5331 - val_sparse_categorical_accuracy: 0.6149 - val_sparse_top_k_categorical_accuracy: 0.7894\n",
            "Epoch 3/10\n",
            "281/281 [==============================] - 9s 32ms/step - loss: 1.2735 - sparse_categorical_accuracy: 0.6669 - sparse_top_k_categorical_accuracy: 0.8316 - val_loss: 1.5295 - val_sparse_categorical_accuracy: 0.6358 - val_sparse_top_k_categorical_accuracy: 0.8063\n",
            "Epoch 4/10\n",
            "281/281 [==============================] - 9s 33ms/step - loss: 0.9871 - sparse_categorical_accuracy: 0.7377 - sparse_top_k_categorical_accuracy: 0.8922 - val_loss: 1.6814 - val_sparse_categorical_accuracy: 0.6273 - val_sparse_top_k_categorical_accuracy: 0.8041\n",
            "Epoch 5/10\n",
            "281/281 [==============================] - 9s 31ms/step - loss: 0.7224 - sparse_categorical_accuracy: 0.8131 - sparse_top_k_categorical_accuracy: 0.9413 - val_loss: 1.9057 - val_sparse_categorical_accuracy: 0.6273 - val_sparse_top_k_categorical_accuracy: 0.8072\n",
            "Epoch 6/10\n",
            "281/281 [==============================] - 9s 31ms/step - loss: 0.5272 - sparse_categorical_accuracy: 0.8686 - sparse_top_k_categorical_accuracy: 0.9654 - val_loss: 2.1612 - val_sparse_categorical_accuracy: 0.6269 - val_sparse_top_k_categorical_accuracy: 0.8059\n",
            "Epoch 7/10\n",
            "281/281 [==============================] - 9s 31ms/step - loss: 0.4073 - sparse_categorical_accuracy: 0.9033 - sparse_top_k_categorical_accuracy: 0.9809 - val_loss: 2.3652 - val_sparse_categorical_accuracy: 0.6260 - val_sparse_top_k_categorical_accuracy: 0.8103\n",
            "Epoch 8/10\n",
            "281/281 [==============================] - 9s 32ms/step - loss: 0.3348 - sparse_categorical_accuracy: 0.9239 - sparse_top_k_categorical_accuracy: 0.9884 - val_loss: 2.5491 - val_sparse_categorical_accuracy: 0.6273 - val_sparse_top_k_categorical_accuracy: 0.8085\n",
            "Epoch 9/10\n",
            "281/281 [==============================] - 9s 31ms/step - loss: 0.2874 - sparse_categorical_accuracy: 0.9337 - sparse_top_k_categorical_accuracy: 0.9916 - val_loss: 2.6866 - val_sparse_categorical_accuracy: 0.6251 - val_sparse_top_k_categorical_accuracy: 0.8085\n",
            "Epoch 10/10\n",
            "281/281 [==============================] - 9s 31ms/step - loss: 0.2548 - sparse_categorical_accuracy: 0.9391 - sparse_top_k_categorical_accuracy: 0.9939 - val_loss: 2.8059 - val_sparse_categorical_accuracy: 0.6273 - val_sparse_top_k_categorical_accuracy: 0.8099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIkv227BPg4"
      },
      "source": [
        "2. Pre-defined train_on_batch method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-4tYurYBK6d",
        "outputId": "1848ecf0-162e-4104-fdc7-5f1c9885b883"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def create_model():\n",
        "  model = models.Sequential()\n",
        "\n",
        "  model.add(layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN))\n",
        "  model.add(layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\"))\n",
        "  model.add(layers.MaxPool1D(2))\n",
        "  model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "  model.add(layers.MaxPool1D(2))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(CAT_NUM, activation=\"softmax\"))\n",
        "  return model\n",
        "\n",
        "def compile_model(model):\n",
        "  model.compile(optimizer=optimizers.Nadam(),\n",
        "                loss=losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)])\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "model = compile_model(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4L2OrY9CaUW"
      },
      "source": [
        "def train_model(model, ds_train, ds_valid, epoches):\n",
        "  for epoch in tf.range(1, epoches+1):\n",
        "    model.reset_metrics()\n",
        "\n",
        "    if epoch == 5:\n",
        "      model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
        "      tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
        "    \n",
        "    for x, y in ds_train:\n",
        "      train_result = model.train_on_batch(x, y)\n",
        "    \n",
        "    for x, y in ds_valid:\n",
        "      valid_result = model.test_on_batch(x, y, reset_metrics=False)\n",
        "    \n",
        "    if epoch % 1 == 0:\n",
        "      tf.print(\"epoch = \", epoch)\n",
        "      print(\"train:\", dict(zip(model.metrics_names, train_result)))\n",
        "      print(\"valid:\", dict(zip(model.metrics_names, valid_result)))\n",
        "      print(\"\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9wGZEZMEFlW",
        "outputId": "097562c3-7bac-485f-9d14-26b605702d47"
      },
      "source": [
        "train_model(model, ds_train, ds_test, 10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch =  1\n",
            "train: {'loss': 1.448559045791626, 'sparse_categorical_accuracy': 0.6363636255264282, 'sparse_top_k_categorical_accuracy': 0.8181818127632141}\n",
            "valid: {'loss': 1.6617642641067505, 'sparse_categorical_accuracy': 0.5672306418418884, 'sparse_top_k_categorical_accuracy': 0.7622439861297607}\n",
            "\n",
            "epoch =  2\n",
            "train: {'loss': 1.1990635395050049, 'sparse_categorical_accuracy': 0.6363636255264282, 'sparse_top_k_categorical_accuracy': 0.9090909361839294}\n",
            "valid: {'loss': 1.5186762809753418, 'sparse_categorical_accuracy': 0.609082818031311, 'sparse_top_k_categorical_accuracy': 0.7983080744743347}\n",
            "\n",
            "epoch =  3\n",
            "train: {'loss': 0.9579431414604187, 'sparse_categorical_accuracy': 0.6818181872367859, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 1.560197353363037, 'sparse_categorical_accuracy': 0.6139804124832153, 'sparse_top_k_categorical_accuracy': 0.7996438145637512}\n",
            "\n",
            "epoch =  4\n",
            "train: {'loss': 0.6208112239837646, 'sparse_categorical_accuracy': 0.7727272510528564, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 1.761997938156128, 'sparse_categorical_accuracy': 0.6175423264503479, 'sparse_top_k_categorical_accuracy': 0.7876224517822266}\n",
            "\n",
            "Lowering optimizer Learning Rate...\n",
            "\n",
            "\n",
            "epoch =  5\n",
            "train: {'loss': 0.39427053928375244, 'sparse_categorical_accuracy': 0.8636363744735718, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 1.9773966073989868, 'sparse_categorical_accuracy': 0.6188780069351196, 'sparse_top_k_categorical_accuracy': 0.7902938723564148}\n",
            "\n",
            "epoch =  6\n",
            "train: {'loss': 0.34298083186149597, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 2.1598074436187744, 'sparse_categorical_accuracy': 0.6153160929679871, 'sparse_top_k_categorical_accuracy': 0.7853962779045105}\n",
            "\n",
            "epoch =  7\n",
            "train: {'loss': 0.3094366788864136, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 2.334057331085205, 'sparse_categorical_accuracy': 0.6135351657867432, 'sparse_top_k_categorical_accuracy': 0.7849510312080383}\n",
            "\n",
            "epoch =  8\n",
            "train: {'loss': 0.2804448902606964, 'sparse_categorical_accuracy': 0.8636363744735718, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 2.500746250152588, 'sparse_categorical_accuracy': 0.6113089919090271, 'sparse_top_k_categorical_accuracy': 0.7853962779045105}\n",
            "\n",
            "epoch =  9\n",
            "train: {'loss': 0.27147114276885986, 'sparse_categorical_accuracy': 0.8636363744735718, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 2.652158737182617, 'sparse_categorical_accuracy': 0.6099733114242554, 'sparse_top_k_categorical_accuracy': 0.7862867116928101}\n",
            "\n",
            "epoch =  10\n",
            "train: {'loss': 0.2747700810432434, 'sparse_categorical_accuracy': 0.8636363744735718, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "valid: {'loss': 2.7917320728302, 'sparse_categorical_accuracy': 0.6077470779418945, 'sparse_top_k_categorical_accuracy': 0.7836152911186218}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSMih6olEcUC"
      },
      "source": [
        "3. Customized Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgvrUM50EbIC",
        "outputId": "d537da6d-0105-4aa7-9f29-48f93973a4d0"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def create_model():\n",
        "  model = models.Sequential()\n",
        "\n",
        "  model.add(layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN))\n",
        "  model.add(layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\"))\n",
        "  model.add(layers.MaxPool1D(2))\n",
        "  model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "  model.add(layers.MaxPool1D(2))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(CAT_NUM, activation=\"softmax\"))\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2YTrnJrFa0T",
        "outputId": "a968ea82-1f4a-4b89-8b0e-bc5c3a402dee"
      },
      "source": [
        "optimizer = optimizers.Nadam()\n",
        "loss_func = losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "train_loss = metrics.Mean(name='train_loss')\n",
        "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "valid_loss = metrics.Mean(name='valid_loss')\n",
        "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, features, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predications = model(features, training=True)\n",
        "    loss = loss_func(labels, predications)\n",
        "  \n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss.update_state(loss)\n",
        "  train_metric.update_state(labels, predications)\n",
        "\n",
        "@tf.function\n",
        "def valid_step(model, features, labels):\n",
        "  predications = model(features)\n",
        "  batch_loss = loss_func(labels, predications)\n",
        "  valid_loss.update_state(batch_loss)\n",
        "  valid_metric.update_state(labels, predications)\n",
        "\n",
        "def train_model(model, ds_train, ds_valid, epochs):\n",
        "  for epoch in tf.range(1, epochs+1):\n",
        "    for features, labels in ds_train:\n",
        "      train_step(model, features, labels)\n",
        "    \n",
        "    for features, labels in ds_valid:\n",
        "      valid_step(model, features, labels)\n",
        "    \n",
        "    logs = 'Epoch={}, Loss:{}, Accuracy:{}, Valid Loss:{}, Valid Accuracy:{}'\n",
        "\n",
        "    if epoch%1 ==0:\n",
        "        tf.print(tf.strings.format(logs,\n",
        "        (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
        "        tf.print(\"\")\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    train_metric.reset_states()\n",
        "    valid_metric.reset_states()\n",
        "\n",
        "train_model(model, ds_train, ds_test, 10)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch=1, Loss:1.53299952, Accuracy:0.606101096, Valid Loss:1.53616142, Valid Accuracy:0.612644672\n",
            "\n",
            "Epoch=2, Loss:1.26489651, Accuracy:0.66444, Valid Loss:1.51128232, Valid Accuracy:0.636687458\n",
            "\n",
            "Epoch=3, Loss:1.00085247, Accuracy:0.73101759, Valid Loss:1.63053751, Valid Accuracy:0.645146906\n",
            "\n",
            "Epoch=4, Loss:0.721742511, Accuracy:0.812402606, Valid Loss:1.83259153, Valid Accuracy:0.644701719\n",
            "\n",
            "Epoch=5, Loss:0.510541916, Accuracy:0.872968137, Valid Loss:2.08863878, Valid Accuracy:0.643811226\n",
            "\n",
            "Epoch=6, Loss:0.390402853, Accuracy:0.905922949, Valid Loss:2.27636456, Valid Accuracy:0.644701719\n",
            "\n",
            "Epoch=7, Loss:0.31842044, Accuracy:0.92351371, Valid Loss:2.40933776, Valid Accuracy:0.642475486\n",
            "\n",
            "Epoch=8, Loss:0.27190575, Accuracy:0.93509239, Valid Loss:2.53042626, Valid Accuracy:0.642030299\n",
            "\n",
            "Epoch=9, Loss:0.239299655, Accuracy:0.941327095, Valid Loss:2.65123105, Valid Accuracy:0.634016037\n",
            "\n",
            "Epoch=10, Loss:0.216556847, Accuracy:0.944778442, Valid Loss:2.7426393, Valid Accuracy:0.627337515\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}