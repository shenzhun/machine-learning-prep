{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day13_dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3+5CwiHIXCDdnrwtM6guC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shenzhun/machine-learning-prep/blob/master/tensorflow/day13_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFMK1D4cd81r",
        "outputId": "14120172-a293-4e72-93cb-502c8e4f1fb1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oURmkZgoe-2_",
        "outputId": "f40e39a2-42f0-4fa7-b303-53815b014dae"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "ds1 = tf.data.Dataset.from_tensor_slices((iris[\"data\"], iris[\"target\"]))\n",
        "\n",
        "for features, label in ds1.take(5):\n",
        "  print(features, label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkhcZS1ye1Wm",
        "outputId": "cfe1c2b0-4434-48a6-bb1e-d5a72439ca85"
      },
      "source": [
        "import pandas as pd\n",
        "dfiris = pd.DataFrame(iris[\"data\"], columns = iris.feature_names)\n",
        "ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(\"list\"),iris[\"target\"]))\n",
        "\n",
        "for features, label in ds2.take(3):\n",
        "  print(features, label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=5.1>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.5>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
            "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.9>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
            "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.7>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.2>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "IXoWccfUgrGy",
        "outputId": "8835e6bb-9006-4459-c321-3d713d8c564a"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "image_generator = ImageDataGenerator(rescale=1.0/255).flow_from_directory(\n",
        "    \"../data/cifar2/test/\",\n",
        "    target_size=(32, 32),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "classdict = image_generator.class_indices\n",
        "print(classidct)\n",
        "\n",
        "def generator():\n",
        "  for features, label in image_generator:\n",
        "    yield features, label\n",
        "\n",
        "ds3 = tf.data.Dataset.from_generator(generator, output_types=(tf.float32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-24ab80688020>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   def flow_from_dataframe(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/cifar2/test/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEvLqm9Ujxs9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "b4ceb3ed-cdb0-4d68-c1e7-024f017051bc"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "plt.figure(figsize=(6,6)) \n",
        "for i,(img,label) in enumerate(ds3.unbatch().take(9)):\n",
        "    ax=plt.subplot(3,3,i+1)\n",
        "    ax.imshow(img.numpy())\n",
        "    ax.set_title(\"label = %d\"%label)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([]) \n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7fdf8156cf1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config InlineBackend.figure_format = 'svg'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ds3' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "64o98HzilIfw",
        "outputId": "230a4214-bdae-48c3-e3d5-d359e98636c8"
      },
      "source": [
        "# Constructing Data Pipeline through csv file\n",
        "ds4 = tf.data.experimental.make_csv_dataset(\n",
        "      file_pattern = [\"../data/titanic/train.csv\",\"../data/titanic/test.csv\"],\n",
        "      batch_size=3, \n",
        "      label_name=\"Survived\",\n",
        "      na_value=\"\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True)\n",
        "\n",
        "for data,label in ds4.take(2):\n",
        "    print(data,label)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f903d3eb4ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       ignore_errors=True)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36mmake_csv_dataset_v2\u001b[0;34m(file_pattern, batch_size, column_names, column_defaults, label_name, select_columns, field_delim, use_quote_delim, na_value, header, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, num_parallel_reads, sloppy, num_rows_for_inference, compression_type, ignore_errors)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;31m# Create dataset of all matching filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m   \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_file_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py\u001b[0m in \u001b[0;36m_get_file_names\u001b[0;34m(file_pattern, shuffle)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No files match %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfile_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m   \u001b[0;31m# Sort files so it will be deterministic for unit tests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No files match ['../data/titanic/train.csv', '../data/titanic/test.csv']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDvsEF9jlfxQ"
      },
      "source": [
        "ds5 = tf.data.TextLineDataset(\n",
        "    filename = [\"../data/titanic/train.csv\",\"../data/titanic/test.csv\"]\n",
        ").skip(1)\n",
        "\n",
        "for line in ds5.take(5):\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm098VyBl-fd"
      },
      "source": [
        "ds6 = tf.data.Dataset.list_files(\n",
        "    ../data/cifar2/train/*/*.jpg\"\n",
        ")\n",
        "for file in ds6.take(5):\n",
        "  print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdo_7_MfmebU"
      },
      "source": [
        "from matplotlib import pyplot as plt \n",
        "def load_image(img_path,size = (32,32)):\n",
        "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img) # Note that we are using jpeg format\n",
        "    img = tf.image.resize(img,size)\n",
        "    return(img,label)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "for i,(img,label) in enumerate(ds6.map(load_image).take(2)):\n",
        "    plt.figure(i)\n",
        "    plt.imshow((img/255.0).numpy())\n",
        "    plt.title(\"label = %d\"%label)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOXxgGbamyQD"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_tfrecords(inpath, outpath):\n",
        "  writer = tf.io.TFRecordWriter(outpath)\n",
        "  dirs = os.listdir(inpath)\n",
        "  for index, name in enumerate(dirs):\n",
        "    class_path = inpath + \"/\" + name + \"/\"\n",
        "    for img_name in os.listdir(class_path):\n",
        "      img_path = class_path + img_name\n",
        "      img = tf.io.read_file(img_path)\n",
        "      example = tf.train.Example(\n",
        "          features = tf.train.Features(feature={\n",
        "              'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
        "              'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))\n",
        "          })\n",
        "      )\n",
        "      writer.write(example.SerializeToString())\n",
        "  writer.close()\n",
        "\n",
        "create_tfrecords(\"../data/cifar2/test/\",\"../data/cifar2_test.tfrecords/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3DaxLRGoNpa"
      },
      "source": [
        "from matplotlib import pyplot as plt \n",
        "\n",
        "def parse_example(proto):\n",
        "    description ={ 'img_raw' : tf.io.FixedLenFeature([], tf.string),\n",
        "                   'label': tf.io.FixedLenFeature([], tf.int64)} \n",
        "    example = tf.io.parse_single_example(proto, description)\n",
        "    img = tf.image.decode_jpeg(example[\"img_raw\"])   # Note that we are using jpeg format\n",
        "    img = tf.image.resize(img, (32,32))\n",
        "    label = example[\"label\"]\n",
        "    return(img,label)\n",
        "\n",
        "ds7 = tf.data.TFRecordDataset(\"../data/cifar2_test.tfrecords\").map(parse_example).shuffle(3000)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "plt.figure(figsize=(6,6)) \n",
        "for i,(img,label) in enumerate(ds7.take(9)):\n",
        "    ax=plt.subplot(3,3,i+1)\n",
        "    ax.imshow((img/255.0).numpy())\n",
        "    ax.set_title(\"label = %d\"%label)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([]) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_iiqSrOocob"
      },
      "source": [
        "Applying Data Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVIsUi8PobZD",
        "outputId": "78e84c7e-c1ae-46f1-8c77-71bd7b210c85"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices([\"hello world\", \"hello China\", \"hello Beijing\"])\n",
        "ds_map = ds.map(lambda x: tf.strings.split(x, \" \"))\n",
        "for x in ds_map:\n",
        "  print(x)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\n",
            "tf.Tensor([b'hello' b'China'], shape=(2,), dtype=string)\n",
            "tf.Tensor([b'hello' b'Beijing'], shape=(2,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HdkD8mLo9JZ",
        "outputId": "d2b7e33c-dc27-4027-98cc-42f2ea9f15c6"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices([\"hello world\", \"hello China\", \"hello Beijing\"])\n",
        "ds_flatmap = ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(tf.strings.split(x, \" \")))\n",
        "for x in ds_flatmap:\n",
        "  print(x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'hello', shape=(), dtype=string)\n",
            "tf.Tensor(b'world', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello', shape=(), dtype=string)\n",
            "tf.Tensor(b'China', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello', shape=(), dtype=string)\n",
            "tf.Tensor(b'Beijing', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0sGu4BWpbg-",
        "outputId": "b6c2b8b9-8a70-4eec-e615-6ebe55961059"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
        "ds_interleave = ds.interleave(lambda x: tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\n",
        "for x in ds_interleave:\n",
        "  print(x)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'hello', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello', shape=(), dtype=string)\n",
            "tf.Tensor(b'world', shape=(), dtype=string)\n",
            "tf.Tensor(b'China', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello', shape=(), dtype=string)\n",
            "tf.Tensor(b'Beijing', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxdtA_wIqrTu",
        "outputId": "fda8e454-54e6-435b-f729-7f7c2c342b45"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n",
        "ds_filter = ds.filter(lambda x: tf.strings.regex_full_match(x, \".*[a|B].*\"))\n",
        "for x in ds_filter:\n",
        "  print(x)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'hello China', shape=(), dtype=string)\n",
            "tf.Tensor(b'hello Beijing', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fDm-kNorbSV",
        "outputId": "f8511d6a-0687-4816-991a-8368c8d6138c"
      },
      "source": [
        "ds1 = tf.data.Dataset.range(0, 3)\n",
        "ds2 = tf.data.Dataset.range(3, 6)\n",
        "ds3 = tf.data.Dataset.range(6, 9)\n",
        "ds_zip = tf.data.Dataset.zip((ds1, ds2, ds3))\n",
        "for x, y, z in ds_zip:\n",
        "  print(x.numpy(), y.numpy(), z.numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3 6\n",
            "1 4 7\n",
            "2 5 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL32FD50r_sD",
        "outputId": "2c206a33-f19e-4c52-9acb-db8d4a3b0936"
      },
      "source": [
        "ds1 = tf.data.Dataset.range(0, 3)\n",
        "ds2 = tf.data.Dataset.range(3, 6)\n",
        "ds_concat = tf.data.Dataset.concatenate(ds1, ds2)\n",
        "for x in ds_concat:\n",
        "  print(x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkQm1ETrsXUo",
        "outputId": "6480b9bf-f40c-4595-b4a1-116ec01647f9"
      },
      "source": [
        "ds = tf.data.Dataset.range(12)\n",
        "ds_batch = ds.batch(4)\n",
        "for x in ds_batch:\n",
        "  print(x)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
            "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
            "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzPiFCqKsmMx",
        "outputId": "dcdb7884-f3c9-4a14-a4ba-46ea6ce138d6"
      },
      "source": [
        "elements = [[1, 2], [3, 4, 5], [6, 7], [8]]\n",
        "ds = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32)\n",
        "\n",
        "ds_padded_batch = ds.padded_batch(2, padded_shapes=[4,])\n",
        "for x in ds_padded_batch:\n",
        "  print(x)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1 2 0 0]\n",
            " [3 4 5 0]], shape=(2, 4), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[6 7 0 0]\n",
            " [8 0 0 0]], shape=(2, 4), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdyTBunNtw7M",
        "outputId": "e63f4ee6-339a-49d3-92c1-f929c272ec46"
      },
      "source": [
        "ds = tf.data.Dataset.range(12)\n",
        "ds_window = ds.window(3, shift=1).flat_map(lambda x: x.batch(3, drop_remainder=True))\n",
        "for x in ds_window:\n",
        "  print(x)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n",
            "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
            "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
            "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n",
            "tf.Tensor([4 5 6], shape=(3,), dtype=int64)\n",
            "tf.Tensor([5 6 7], shape=(3,), dtype=int64)\n",
            "tf.Tensor([6 7 8], shape=(3,), dtype=int64)\n",
            "tf.Tensor([7 8 9], shape=(3,), dtype=int64)\n",
            "tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)\n",
            "tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEfRoA1cuR0u",
        "outputId": "d22661e5-b113-4481-90b8-da4bf0a70c7b"
      },
      "source": [
        "#shuffle: shuffling the order of the data.\n",
        "\n",
        "ds = tf.data.Dataset.range(12)\n",
        "ds_shuffle = ds.shuffle(buffer_size=5)\n",
        "for x in ds_shuffle:\n",
        "  print(x)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(10, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(11, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx-Puy1SutQd",
        "outputId": "23942cf2-8936-4323-9268-aec6f41edcae"
      },
      "source": [
        "# repeat: repeat the data centain times\n",
        "\n",
        "ds = tf.data.Dataset.range(3)\n",
        "ds_repeat = ds.repeat(3)\n",
        "for x in ds_repeat:\n",
        "  print(x)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lw9Fdm-vGWh",
        "outputId": "c00dab51-1f0d-4aa8-b2d0-dfab332b0a27"
      },
      "source": [
        "#shard: sampling the elements starting from a certain position with fixed distance.\n",
        "\n",
        "ds = tf.data.Dataset.range(12)\n",
        "ds_shard = ds.shard(3, index = 1)\n",
        "for x in ds_shard:\n",
        "  print(x)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(10, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_9fr1kQvk-w",
        "outputId": "c951d365-4259-4d0d-82bd-2d364af6119c"
      },
      "source": [
        "#take: sampling the first few elements from a certain position.\n",
        "\n",
        "ds = tf.data.Dataset.range(12)\n",
        "ds_take = ds.take(3)\n",
        "\n",
        "list(ds_take.as_numpy_iterator())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNodyh9swKlc"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def printbar():\n",
        "  ts = tf.timestamp()\n",
        "  today_ts = ts % (24 * 60 * 60)\n",
        "\n",
        "  hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
        "  minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
        "  second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
        "\n",
        "  def timeformat(m):\n",
        "    if tf.strings.length(tf.strings.format(\"{}\", m)) == 1:\n",
        "      return tf.strings.format(\"0{}\", m)\n",
        "    else:\n",
        "      return tf.strings.format(\"{}\", m)\n",
        "  \n",
        "  timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
        "                               timeformat(second)], separator=\":\")\n",
        "  tf.print(\"=========\" * 8, end = \"\")\n",
        "  tf.print(timestring)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjhXSGlgxP5n"
      },
      "source": [
        "import time\n",
        "\n",
        "def generator():\n",
        "  for i in range(10):\n",
        "    time.sleep(2)\n",
        "    yield i\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(generator, output_types=tf.int32)\n",
        "\n",
        "def train_step():\n",
        "  time.sleep(1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBmQ_6Xux4Gj",
        "outputId": "7e0d2d3e-2a71-4f01-924b-422d8353299b"
      },
      "source": [
        "# Estimated time of training: 10*2+10*1 = 30s\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start training...\"))\n",
        "for x in ds:\n",
        "    train_step()  \n",
        "printbar()\n",
        "tf.print(tf.constant(\"end training...\"))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================14:12:56\n",
            "start training...\n",
            "========================================================================14:13:26\n",
            "end training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzmLYKkGx_3N",
        "outputId": "bc2387af-cc9b-4ba1-f22d-4a42c9d8f3fd"
      },
      "source": [
        "# Use method prefetch to parallel the processes of data preparation and parameter iteration.\n",
        "\n",
        "# Estimated time of training:  max(10*2,10*1) = 20s\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start training with prefetch...\"))\n",
        "\n",
        "# tf.data.experimental.AUTOTUNE allows auto-selection of parameters\n",
        "for x in ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):\n",
        "    train_step()  \n",
        "    \n",
        "printbar()\n",
        "tf.print(tf.constant(\"end training...\"))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================14:13:27\n",
            "start training with prefetch...\n",
            "========================================================================14:13:48\n",
            "end training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "2QCXGvUmyrpq",
        "outputId": "3b8e6bf8-5428-41f4-d342-e00286a3bcc4"
      },
      "source": [
        "ds_files = tf.data.Dataset.list_files(\"../data/titanic/*.csv\")\n",
        "ds = ds_files.flat_map(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
        "for line in ds.take(4):\n",
        "    print(line)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-20d4a7c99062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/titanic/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0;32m-> 1225\u001b[0;31m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[0m\u001b[1;32m   1226\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mmatching_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;34m\"\"\"Decorates the input function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[1;32m    248\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                                      error_in_function=error_in_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m           \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Expected '%s' to be true. Summarized data: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m           (condition, \"\\n\".join(data_str)))\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ../data/titanic/*.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "QRFZWbktyulK",
        "outputId": "29ac22a2-39f5-475a-ee9f-320cf444d3c6"
      },
      "source": [
        "ds_files = tf.data.Dataset.list_files(\"../data/titanic/*.csv\")\n",
        "ds = ds_files.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))\n",
        "for line in ds.take(8):\n",
        "    print(line)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6f4ef806bd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/titanic/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0;32m-> 1225\u001b[0;31m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[0m\u001b[1;32m   1226\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mmatching_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;34m\"\"\"Decorates the input function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[1;32m    248\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                                      error_in_function=error_in_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m           \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Expected '%s' to be true. Summarized data: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m           (condition, \"\\n\".join(data_str)))\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ../data/titanic/*.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn1iK5uHy-VZ"
      },
      "source": [
        "ds = tf.data.Dataset.list_files(\"../data/cifar2/train/*/*.jpg\")\n",
        "def load_image(img_path,size = (32,32)):\n",
        "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img) #Note: jpeg format here\n",
        "    img = tf.image.resize(img,size)\n",
        "    return(img,label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HceLXD3zy_nx"
      },
      "source": [
        "# Conversion with single process\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start transformation...\"))\n",
        "\n",
        "ds_map = ds.map(load_image)\n",
        "for _ in ds_map:\n",
        "    pass\n",
        "\n",
        "printbar()\n",
        "tf.print(tf.constant(\"end transformation...\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgE3r0lgzG2z"
      },
      "source": [
        "# Conversion with multi-process\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start parallel transformation...\"))\n",
        "\n",
        "ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "for _ in ds_map_parallel:\n",
        "    pass\n",
        "\n",
        "printbar()\n",
        "tf.print(tf.constant(\"end parallel transformation...\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNpbIKgkzX0n",
        "outputId": "ae7b415a-632b-4c3f-9c7e-246fe0efb0f6"
      },
      "source": [
        "import time\n",
        "\n",
        "def generator():\n",
        "  for i in range(5):\n",
        "    time.sleep(2)\n",
        "    yield i\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(generator, output_types= (tf.int32))\n",
        "\n",
        "def train_step():\n",
        "  pass\n",
        "\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start training...\"))\n",
        "\n",
        "for epoch in tf.range(3):\n",
        "  for x in ds:\n",
        "    train_step()\n",
        "  \n",
        "  printbar()\n",
        "  tf.print(\"epoch =\", epoch, \" ended\")\n",
        "printbar()\n",
        "tf.print(tf.constant(\"end training..\"))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================14:25:55\n",
            "start training...\n",
            "========================================================================14:26:05\n",
            "epoch = 0  ended\n",
            "========================================================================14:26:15\n",
            "epoch = 1  ended\n",
            "========================================================================14:26:25\n",
            "epoch = 2  ended\n",
            "========================================================================14:26:25\n",
            "end training..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PboHhUpT0426",
        "outputId": "7bd2a6a6-5bf9-4316-f8b1-bdc0fc7fb4cf"
      },
      "source": [
        "import time\n",
        "\n",
        "def generator():\n",
        "  for i in range(5):\n",
        "    time.sleep(2)\n",
        "    yield i\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(generator, output_types = (tf.int32)).cache()\n",
        "\n",
        "def train_step():\n",
        "  time.sleep(0)\n",
        "\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start training...\"))\n",
        "for epoch in tf.range(3):\n",
        "  for x in ds:\n",
        "    train_step()\n",
        "  \n",
        "  printbar()\n",
        "  tf.print(\"epoch =\", epoch, \" ended\")\n",
        "printbar()\n",
        "tf.print(tf.constant(\"end training...\"))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================14:28:59\n",
            "start training...\n",
            "========================================================================14:29:09\n",
            "epoch = 0  ended\n",
            "========================================================================14:29:09\n",
            "epoch = 1  ended\n",
            "========================================================================14:29:09\n",
            "epoch = 2  ended\n",
            "========================================================================14:29:09\n",
            "end training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNlqtE4N1s58",
        "outputId": "aa50584a-d3d6-42e8-f2ea-5bf14c454c7f"
      },
      "source": [
        "ds = tf.data.Dataset.range(100000)\n",
        "ds_map_batch = ds.map(lambda x: x**2).batch(20)\n",
        "\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start scalar transoformation...\"))\n",
        "for x in ds_map_batch:\n",
        "  pass\n",
        "\n",
        "printbar()\n",
        "tf.print(tf.constant(\"end scalar transformation...\"))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================14:31:05\n",
            "start scalar transoformation...\n",
            "========================================================================14:31:08\n",
            "end scalar transformation...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alz_lvpe1cXm",
        "outputId": "fee1219a-946b-43be-85d1-25e19afb8a82"
      },
      "source": [
        "ds = tf.data.Dataset.range(100000)\n",
        "ds_batch_map = ds.batch(20).map(lambda x: x**2)\n",
        "\n",
        "printbar()\n",
        "tf.print(tf.constant(\"start vector transformation...\"))\n",
        "for x in ds_batch_map:\n",
        "  pass\n",
        "printbar()\n",
        "tf.print(tf.constant(\"end vecotr transformation...\"))\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================14:32:36\n",
            "start vector transformation...\n",
            "========================================================================14:32:37\n",
            "end vecotr transformation...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}