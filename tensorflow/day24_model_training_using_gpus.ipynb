{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day24_model_training_using_gpus.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhfiCu5de9g8oc6NlXgxEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shenzhun/machine-learning-prep/blob/master/tensorflow/day24_model_training_using_gpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaZBkWi-5ltC",
        "outputId": "d99ba6a1-f2e8-455e-86d2-13d748d498b0"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras import * "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zolxRVCx6Pgo"
      },
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "  gpu0 = gpus[0]\n",
        "  tf.config.experimental.set_memory_growth(gpu0, True)\n",
        "  # tf.config.experimental.set_virtual_device_configuration(gpu0, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
        "  tf.config.set_visible_devices([gpu0], \"GPU\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5bowxcg6_uN",
        "outputId": "d89d4927-89e8-446a-d9a0-479589a9a69c"
      },
      "source": [
        "with tf.device(\"/gpu:0\"):\n",
        "  tf.random.set_seed(0)\n",
        "  a = tf.random.uniform((10000, 100), minval=0, maxval=3.0)\n",
        "  b = tf.random.uniform((100, 100000), minval=0, maxval=3.0)\n",
        "  c = a@b\n",
        "  tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=0), axis=0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.24953778e+11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZaUSZia7y2x",
        "outputId": "b8e23b3b-1bc1-4378-e13b-95c9da766365"
      },
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "  tf.random.set_seed(0)\n",
        "  a = tf.random.uniform((10000,100), minval=0, maxval=3.0)\n",
        "  b = tf.random.uniform((100, 100000), minval=0, maxval=3.0)\n",
        "  c = a@b\n",
        "  tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=0), axis=0))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.24953778e+11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQqtVDRe9BG8",
        "outputId": "84ca270f-29e5-4e1b-a2ce-0f98dcdc8737"
      },
      "source": [
        "MAX_LEN = 300\n",
        "BATCH_SIZE = 32\n",
        "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n",
        "\n",
        "MAX_WORDS = x_train.max()+1\n",
        "CAT_NUM = y_train.max()+1\n",
        "\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n",
        "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
        "   \n",
        "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n",
        "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n",
        "          "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjD29_lW9QZq",
        "outputId": "84d5a126-75ba-4621-8dcc-df1de494aa86"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def create_model():\n",
        "    \n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
        "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
        "    model.add(layers.MaxPool1D(2))\n",
        "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
        "    model.add(layers.MaxPool1D(2))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n",
        "    return(model)\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BfbLOtY9XtD"
      },
      "source": [
        "optimizer = optimizers.Nadam()\n",
        "loss_func = losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "train_loss = metrics.Mean(name='train_loss')\n",
        "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "valid_loss = metrics.Mean(name='valid_loss')\n",
        "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(features,training = True)\n",
        "        loss = loss_func(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss.update_state(loss)\n",
        "    train_metric.update_state(labels, predictions)\n",
        "    \n",
        "@tf.function\n",
        "def valid_step(model, features, labels):\n",
        "    predictions = model(features)\n",
        "    batch_loss = loss_func(labels, predictions)\n",
        "    valid_loss.update_state(batch_loss)\n",
        "    valid_metric.update_state(labels, predictions)\n",
        "    \n",
        "\n",
        "def train_model(model,ds_train,ds_valid,epochs):\n",
        "    for epoch in tf.range(1,epochs+1):\n",
        "        \n",
        "        for features, labels in ds_train:\n",
        "            train_step(model,features,labels)\n",
        "\n",
        "        for features, labels in ds_valid:\n",
        "            valid_step(model,features,labels)\n",
        "\n",
        "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
        "        \n",
        "        if epoch%1 ==0:\n",
        "            printbar()\n",
        "            tf.print(tf.strings.format(logs,\n",
        "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
        "            tf.print(\"\")\n",
        "            \n",
        "        train_loss.reset_states()\n",
        "        valid_loss.reset_states()\n",
        "        train_metric.reset_states()\n",
        "        valid_metric.reset_states()\n",
        "\n",
        "train_model(model,ds_train,ds_test,10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}